# SDA Kolmogorov Flow: Model Architecture

Score-based Data Assimilation (SDA) for Kolmogorov Flow ã®è©³ç´°ãªãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è§£èª¬

---

## ğŸ“‹ ç›®æ¬¡

1. [å…¨ä½“æ§‹é€ ](#å…¨ä½“æ§‹é€ )
2. [æ™‚é–“åŸ‹ã‚è¾¼ã¿ (TimeEmbedding)](#æ™‚é–“åŸ‹ã‚è¾¼ã¿-timeembedding)
3. [U-Net ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£](#u-net-ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£)
4. [Markov Chain Score Network](#markov-chain-score-network)
5. [VPSDE (Variance Preserving SDE)](#vpsde-variance-preserving-sde)
6. [Kolmogorov Flow å›ºæœ‰ã®è¨­è¨ˆ](#kolmogorov-flow-å›ºæœ‰ã®è¨­è¨ˆ)
7. [å­¦ç¿’ã¨ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°](#å­¦ç¿’ã¨ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°)

---

## å…¨ä½“æ§‹é€ 

### ãƒ¬ã‚¤ãƒ¤ãƒ¼æ§‹æˆ

```
Input: x (B, L, C, H, W)  â† æ™‚ç³»åˆ—ã®é€Ÿåº¦å ´
  â†“
MCScoreNet (Markov Chain ãƒ©ãƒƒãƒ‘ãƒ¼)
  â”œâ”€ unfold: (B, L, C, H, W) â†’ (B, L', C*(2*order+1), H, W)
  â†“
  â””â”€ kernel: LocalScoreUNet (å®Ÿéš›ã®ã‚¹ã‚³ã‚¢é–¢æ•°)
      â”œâ”€ TimeEmbedding: t â†’ time_emb (embeddingæ¬¡å…ƒ)
      â”œâ”€ Forcing: sin(4x) ã‚’ constant channel ã¨ã—ã¦è¿½åŠ 
      â””â”€ UNet: éšå±¤çš„ãªç•³ã¿è¾¼ã¿ãƒãƒƒãƒˆ
          â”œâ”€ Encoder (descent): 3æ®µéšã®ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
          â”œâ”€ Bottleneck: æœ€æ·±å±¤ã®ç‰¹å¾´æŠ½å‡º
          â””â”€ Decoder (ascent): 3æ®µéšã®ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° + skip connections
  â†“
  â”œâ”€ fold: (B, L', C*(2*order+1), H, W) â†’ (B, L, C, H, W)
  â†“
Output: score (B, L, C, H, W)  â† æ¨å®šã•ã‚ŒãŸã‚¹ã‚³ã‚¢é–¢æ•°
```

---

## æ™‚é–“åŸ‹ã‚è¾¼ã¿ (TimeEmbedding)

### ç›®çš„
æ‹¡æ•£éç¨‹ã®æ™‚åˆ» `t âˆˆ [0, 1]` ã‚’ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«æ¡ä»¶ä»˜ã‘ã‚‹ãŸã‚ã®é«˜æ¬¡å…ƒè¡¨ç¾ã‚’ç”Ÿæˆ

### å®Ÿè£…

```python
class TimeEmbedding(nn.Sequential):
    def __init__(self, features: int):
        super().__init__(
            nn.Linear(32, 256),
            nn.SiLU(),
            nn.Linear(256, features),
        )
        self.register_buffer('freqs', torch.pi * torch.arange(1, 16 + 1))

    def forward(self, t: Tensor) -> Tensor:
        # Sinusoidal encoding
        t = self.freqs * t.unsqueeze(dim=-1)  # (B,) â†’ (B, 16)
        t = torch.cat((t.cos(), t.sin()), dim=-1)  # â†’ (B, 32)

        # MLP projection
        return super().forward(t)  # â†’ (B, features)
```

### ç‰¹å¾´

1. **Sinusoidal Encoding**:
   - å‘¨æ³¢æ•°: `Ï€, 2Ï€, 3Ï€, ..., 16Ï€`
   - cos ã¨ sin ã®ä¸¡æ–¹ã‚’ä½¿ç”¨ â†’ 32æ¬¡å…ƒ

2. **MLP Projection**:
   - 32 â†’ 256 â†’ features (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 64)
   - SiLU æ´»æ€§åŒ–é–¢æ•°

3. **åˆ©ç‚¹**:
   - æ™‚é–“ã®é€£ç¶šæ€§ã‚’ä¿æŒ
   - ç•°ãªã‚‹æ™‚é–“ã‚¹ã‚±ãƒ¼ãƒ«ã‚’æ‰ãˆã‚‹
   - Transformer ã® positional encoding ã¨é¡ä¼¼

---

## U-Net ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### åŸºæœ¬æ§‹é€ 

```
å…¥åŠ›: (B, C_in, H, W)
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Encoder (Descent Path)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Level 0: C_in â†’ 96                      â”‚  â† skip_0
â”‚   â”œâ”€ Conv2d(3x3)                        â”‚
â”‚   â””â”€ ResBlock x 3                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Level 1: 96 â†’ 192 (stride=2)            â”‚  â† skip_1
â”‚   â”œâ”€ Conv2d(3x3, stride=2)              â”‚
â”‚   â””â”€ ResBlock x 3                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Level 2: 192 â†’ 384 (stride=2)           â”‚  â† skip_2
â”‚   â”œâ”€ Conv2d(3x3, stride=2)              â”‚
â”‚   â””â”€ ResBlock x 3                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Decoder (Ascent Path)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Level 2: 384 â†’ 192                      â”‚
â”‚   â”œâ”€ ResBlock x 3                       â”‚
â”‚   â”œâ”€ Upsample(2x) + Conv2d              â”‚
â”‚   â””â”€ + skip_2                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Level 1: 192 â†’ 96                       â”‚
â”‚   â”œâ”€ ResBlock x 3                       â”‚
â”‚   â”œâ”€ Upsample(2x) + Conv2d              â”‚
â”‚   â””â”€ + skip_1                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Level 0: 96 â†’ C_out                     â”‚
â”‚   â”œâ”€ ResBlock x 3                       â”‚
â”‚   â””â”€ Conv2d(3x3)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
å‡ºåŠ›: (B, C_out, H, W)
```

### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è©³ç´°

**Kolmogorov ã® CONFIG:**
```python
{
    'window': 5,                      # Markov chain ã® window ã‚µã‚¤ã‚º
    'embedding': 64,                  # æ™‚é–“åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ
    'hidden_channels': (96, 192, 384),  # å„éšå±¤ã®ãƒãƒ£ãƒãƒ«æ•°
    'hidden_blocks': (3, 3, 3),       # å„éšå±¤ã® ResBlock æ•°
    'kernel_size': 3,                 # ç•³ã¿è¾¼ã¿ã‚«ãƒ¼ãƒãƒ«ã‚µã‚¤ã‚º
    'activation': 'SiLU',             # æ´»æ€§åŒ–é–¢æ•°
    'spatial': 2,                     # ç©ºé–“æ¬¡å…ƒ (2D)
    'padding_mode': 'circular',       # å‘¨æœŸå¢ƒç•Œæ¡ä»¶
}
```

### ModResidualBlock

å„ ResBlock ã¯ **æ™‚é–“åŸ‹ã‚è¾¼ã¿ã«ã‚ˆã‚‹å¤‰èª¿** (modulation) ã‚’æŒã¤ï¼š

```python
class ModResidualBlock(nn.Module):
    def __init__(self, project: nn.Module, residue: nn.Module):
        self.project = project  # time_emb â†’ channel_scale
        self.residue = residue  # ç•³ã¿è¾¼ã¿å±¤

    def forward(self, x: Tensor, y: Tensor) -> Tensor:
        # y = time_emb (B, embedding)
        # project(y) â†’ (B, C, 1, 1)
        return x + self.residue(x + self.project(y))
```

**æ§‹é€ :**
```
x (B, C, H, W)
  â”œâ”€ project(time_emb) â†’ (B, C, 1, 1)
  â”‚
  â”œâ”€ + ã§åŠ ç®—
  â†“
  LayerNorm
  â†“
  Conv2d(C, C, 3x3)
  â†“
  SiLU
  â†“
  Conv2d(C, C, 3x3)
  â†“
  + æ®‹å·®æ¥ç¶š
  â†“
å‡ºåŠ› (B, C, H, W)
```

---

## Markov Chain Score Network

### MCScoreNet ã®å½¹å‰²

æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ `x = (x_0, x_1, ..., x_L)` ã«å¯¾ã—ã¦ã€Markovæ€§ã‚’åˆ©ç”¨ã—ã¦ã‚¹ã‚³ã‚¢é–¢æ•°ã‚’åŠ¹ç‡çš„ã«è¨ˆç®—ã€‚

### Unfold/Fold æ“ä½œ

**Unfold: æ™‚é–“çª“ã‚’ä½œæˆ**
```python
@staticmethod
def unfold(x: Tensor, order: int) -> Tensor:
    # x: (B, L, C, H, W)
    # order = 2 (window=5 ã®å ´åˆ)

    x = x.unfold(1, 2 * order + 1, 1)  # (B, L-4, C, H, W, 5)
    x = x.movedim(-1, 2)               # (B, L-4, 5, C, H, W)
    x = x.flatten(2, 3)                # (B, L-4, 5*C, H, W)

    return x
```

**ä¾‹ (window=5, order=2):**
```
å…¥åŠ›: x = [x_0, x_1, x_2, x_3, x_4, x_5, x_6]

unfold å¾Œ:
  ä½ç½® 0: [x_0, x_1, x_2, x_3, x_4]  â† 5ã¤ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’1ã¤ã®ãƒãƒ£ãƒãƒ«ã«
  ä½ç½® 1: [x_1, x_2, x_3, x_4, x_5]
  ä½ç½® 2: [x_2, x_3, x_4, x_5, x_6]

â†’ (B, 3, 10, 64, 64)  â† ãƒãƒ£ãƒãƒ«æ•° = 5 * 2 = 10
```

**Fold: å…ƒã®æ™‚ç³»åˆ—ã«æˆ»ã™**
```python
@staticmethod
def fold(x: Tensor, order: int) -> Tensor:
    # x: (B, L-4, 5*C, H, W)

    x = x.unflatten(2, (2 * order + 1, -1))  # (B, L-4, 5, C, H, W)

    # ç«¯ã®å‡¦ç† + ä¸­å¤®ã®å…¨ãƒ•ãƒ¬ãƒ¼ãƒ 
    return torch.cat((
        x[:, 0, :order],      # æœ€åˆã® order å€‹
        x[:, :, order],       # ã™ã¹ã¦ã®ä¸­å¤®ãƒ•ãƒ¬ãƒ¼ãƒ 
        x[:, -1, -order:],    # æœ€å¾Œã® order å€‹
    ), dim=1)
```

---

## VPSDE (Variance Preserving SDE)

### æ•°å­¦çš„å®šç¾©

**Forward SDE:**
```
dx(t) = -Î²(t)/2 * x(t) dt + âˆšÎ²(t) dw
```

**Perturbation Kernel:**
```
p(x(t) | x) = N(x(t) | Î¼(t)x, ÏƒÂ²(t)I)

Î¼(t) = Î±(t)
ÏƒÂ²(t) = 1 - Î±Â²(t) + Î·Â²
```

**Î±(t) ã®é¸æŠ:**
```python
if alpha == 'cos':  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    Î±(t) = cosÂ²(arccos(âˆšÎ·) * t)
```

### Denoising Score Matching

**å­¦ç¿’ç›®æ¨™:**
```
L = E_{x, t, Îµ} [||Îµ_Î¸(x(t), t) - Îµ||Â²]

where:
  x(t) = Î¼(t)x + Ïƒ(t)Îµ
  Îµ ~ N(0, I)
```

**å®Ÿè£…:**
```python
def loss(self, x: Tensor, c: Tensor = None) -> Tensor:
    t = torch.rand(x.shape[0])  # (B,) â† [0, 1] ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ 

    # Forward diffusion
    eps = torch.randn_like(x)
    x_t = self.mu(t) * x + self.sigma(t) * eps

    # Predict noise
    eps_pred = self.eps(x_t, t, c)

    # MSE loss
    return (eps_pred - eps).square().mean()
```

### Predictor-Corrector ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

**Predictor (Reverse SDE):**
```python
r = Î¼(t - dt) / Î¼(t)
x = r * x + (Ïƒ(t - dt) - r * Ïƒ(t)) * Îµ_Î¸(x, t)
```

**Corrector (Langevin MCMC):**
```python
for _ in range(corrections):
    eps = Îµ_Î¸(x, t - dt)
    Î´ = Ï„ / eps.square().mean()
    x = x - (Î´ * eps + âˆš(2Î´) * z) * Ïƒ(t - dt)
```

---

## Kolmogorov Flow å›ºæœ‰ã®è¨­è¨ˆ

### LocalScoreUNet: Forcing Channel ã®è¿½åŠ 

**Kolmogorov forcing:**
```python
class LocalScoreUNet(ScoreUNet):
    def __init__(self, channels: int, size: int = 64, **kwargs):
        super().__init__(channels, 1, **kwargs)  # context=1

        # sin(4x) ã® forcing ã‚’ä½œæˆ
        domain = 2 * Ï€ / size * (torch.arange(size) + 0.5)
        forcing = torch.sin(4 * domain).expand(1, size, size)

        self.register_buffer('forcing', forcing)

    def forward(self, x: Tensor, t: Tensor, c: Tensor = None) -> Tensor:
        return super().forward(x, t, self.forcing)  # forcing ã‚’ context ã¨ã—ã¦æ¸¡ã™
```

**Forcing ã®å½¹å‰²:**
- Kolmogorov flow ã®å¤–åŠ›é … `f = sin(4y)` ã‚’è¡¨ç¾
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒç‰©ç†çš„ãªå¯¾ç§°æ€§ã‚’å­¦ç¿’ã—ã‚„ã™ãã™ã‚‹
- context channel ã¨ã—ã¦ ScoreUNet ã«å…¥åŠ›

### Circular Padding

**å‘¨æœŸå¢ƒç•Œæ¡ä»¶:**
```python
UNet(..., padding_mode='circular')
```

**åŠ¹æœ:**
- æµä½“ã®å‘¨æœŸæ€§ã‚’ä¿æŒ
- å¢ƒç•Œã§ã®ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã‚’é˜²ã
- ç‰©ç†çš„ã«æ­£ã—ã„å¢ƒç•Œæ¡ä»¶

---

## å­¦ç¿’ã¨ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

### å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹

**1. ãƒ‡ãƒ¼ã‚¿æº–å‚™**
```python
trainset = TrajectoryDataset(PATH / 'data/train.h5', window=5, flatten=True)
# å„ã‚µãƒ³ãƒ—ãƒ«: (64, 2, 64, 64)
#             window*2 ãƒãƒ£ãƒãƒ«, H, W
```

**2. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰**
```python
score = make_score(
    window=5,
    embedding=64,
    hidden_channels=(96, 192, 384),
    hidden_blocks=(3, 3, 3),
)

sde = VPSDE(score.kernel, shape=(10, 64, 64))
```

**3. å­¦ç¿’ãƒ«ãƒ¼ãƒ—**
```python
for epoch in range(epochs):
    for x, _ in trainloader:
        loss = sde.loss(x).mean()

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

### ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ—ãƒ­ã‚»ã‚¹

**1. åˆæœŸãƒã‚¤ã‚º**
```python
x = torch.randn(batch, 10, 64, 64).cuda()  # ~ N(0, I)
```

**2. Reverse SDE**
```python
time = torch.linspace(1, 0, steps + 1)

for t in time[:-1]:
    # Predictor
    r = Î¼(t - dt) / Î¼(t)
    x = r * x + (Ïƒ(t - dt) - r * Ïƒ(t)) * score.kernel(x, t)

    # Corrector (Langevin)
    for _ in range(corrections):
        eps = score.kernel(x, t - dt)
        Î´ = Ï„ / eps.square().mean()
        x = x - (Î´ * eps + âˆš(2Î´) * z) * Ïƒ(t - dt)
```

**3. Unflatten**
```python
x = x.unflatten(1, (-1, 2))  # (B, 10, 64, 64) â†’ (B, 5, 2, 64, 64)
#                                 ãƒãƒ£ãƒãƒ« â†’ (window, u/v, H, W)
```

---

## ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®è¨ˆç®—

### Kolmogorov è¨­å®šã§ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°

**TimeEmbedding:**
```
32 * 256 + 256 * 64 = 24,576
```

**UNet:**

**Level 0 (96 channels):**
- head: `10 * 96 * 3 * 3 = 8,640`
- ResBlock x 3: `(96 * 96 * 3 * 3 * 2) * 3 â‰ˆ 497,664`
- tail: `96 * 10 * 3 * 3 = 8,640`

**Level 1 (192 channels):**
- downconv: `96 * 192 * 3 * 3 = 165,888`
- ResBlock x 3: `(192 * 192 * 3 * 3 * 2) * 3 â‰ˆ 1,990,656`
- upconv: `192 * 96 * 3 * 3 = 165,888`

**Level 2 (384 channels):**
- downconv: `192 * 384 * 3 * 3 = 663,552`
- ResBlock x 3: `(384 * 384 * 3 * 3 * 2) * 3 â‰ˆ 7,962,624`
- upconv: `384 * 192 * 3 * 3 = 663,552`

**Total: ç´„ 12M ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**

---

## ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼å…¨ä½“åƒ

```
[å­¦ç¿’æ™‚]
x (B, 64, 2, 64, 64)  â† HDF5 ã‹ã‚‰èª­ã¿è¾¼ã¿
  â†“ flatten
x (B, 10, 64, 64)  â† 5 window * 2 channels
  â†“ random t ~ U[0,1]
  â†“ forward diffusion
x_t = Î¼(t)x + Ïƒ(t)Îµ
  â†“
MCScoreNet:
  â”œâ”€ unfold â†’ (B, L', 10, 64, 64)
  â”œâ”€ LocalScoreUNet(x_t, t, forcing)
  â”‚   â”œâ”€ TimeEmbedding(t) â†’ (B, 64)
  â”‚   â”œâ”€ cat([x_t, forcing], dim=1) â†’ (B, 11, 64, 64)
  â”‚   â””â”€ UNet(x_cat, time_emb) â†’ (B, 10, 64, 64)
  â””â”€ fold â†’ (B, 10, 64, 64)
  â†“
Îµ_pred (B, 10, 64, 64)
  â†“
loss = ||Îµ_pred - Îµ||Â²


[ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ™‚]
x ~ N(0, I)  (B, 10, 64, 64)
  â†“
for t in [1.0 â†’ 0.0]:
  â”œâ”€ Îµ_pred = LocalScoreUNet(x, t, forcing)
  â”œâ”€ x = r*x + (Ïƒ_new - r*Ïƒ)*Îµ_pred  (Predictor)
  â””â”€ x = x - Î´*Îµ_pred + âˆš(2Î´)*z  (Corrector)
  â†“
x (B, 10, 64, 64)
  â†“ unflatten
x (B, 5, 2, 64, 64)  â† ç”Ÿæˆã•ã‚ŒãŸé€Ÿåº¦å ´
```

---

## ã¾ã¨ã‚

### ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ

1. **MCScoreNet**: Markov chain ã®æ™‚ç³»åˆ—æ§‹é€ ã‚’æ‰±ã†
2. **LocalScoreUNet**: Forcing channel ä»˜ãã® U-Net
3. **TimeEmbedding**: Sinusoidal encoding + MLP
4. **VPSDE**: Variance Preserving SDE ã«ã‚ˆã‚‹ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°
5. **Circular Padding**: å‘¨æœŸå¢ƒç•Œæ¡ä»¶

### è¨­è¨ˆæ€æƒ³

- **ç‰©ç†çš„åˆ¶ç´„ã®çµ„ã¿è¾¼ã¿**: Forcing term, circular padding
- **éšå±¤çš„ç‰¹å¾´æŠ½å‡º**: U-Net ã® encoder-decoder
- **æ™‚é–“æ¡ä»¶ä»˜ã‘**: TimeEmbedding + modulation
- **Markovæ€§ã®æ´»ç”¨**: unfold/fold ã§åŠ¹ç‡çš„ãªæ™‚ç³»åˆ—å‡¦ç†

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: ç´„ 12M
- å…¥åŠ›è§£åƒåº¦: 64Ã—64
- æ™‚é–“çª“: 5 frames
- å­¦ç¿’æ™‚é–“: ~24æ™‚é–“ (4096 epochs, GPU)

ã“ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã‚ˆã‚Šã€Kolmogorov flow ã®è¤‡é›‘ãªéç·šå½¢ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã‚’å­¦ç¿’ã—ã€ãƒ‡ãƒ¼ã‚¿åŒåŒ–ã‚¿ã‚¹ã‚¯ã«é©ç”¨ã§ãã¾ã™ã€‚
